---
layout: post
title: Project - Chile Protests
---

## 2019-20 Chile Protests

### What Happened: 
- Modest hike in subway fare (around 4%) in Santiago in October 2019
- Started as a student led protest the week of October 14
  - By late October, over 1 million people protesting in the streets of Santiago (the largest protests in Chilean history)
  - Protests quickly spread across the country
- No single leader or cause
- Protests have resulted in thousands of injuries and over 30 deaths
- Key focus areas seem to be high levels of income inequality and the cost of services, such as healthcare and education, that were privatized during the Pinochet dictatorship
- The government was caught off guard by the protests—just days before, President Piñera was quoted saying Chile was an “oasis of calm" among its Latin American neighbors

### Key Questions: 
  > - Chile is often called Latin America's Poster Child due to its neoliberal economic model, rapid economic growth, *relative* government stability, and low levels of absolute poverty.
  > - So why did mass protests break out across the country over a small rise in the cost of a metro ticket in Santiago?
  > - Could data have been used to "predict" or anticipate this crisis?
  > - Is this a "revolution of rising expectations"?  

### Methods:
For this project, I simply wanted to explore the available data using new tools and techniques learned in this class, including Git, Python, and R.

#### Part A: Visualization of Macro Data
###### Goal: 
Exploratory data analysis to
- Determine if there are any expected or unexpected relationships between the variables that should be explored further
- Develop time series model to analyze the relationship (if any) between copper prices and protest activity (in progress)
  
  - Role of Fiscal Stabilizer: Given Chile’s reliance on natural resource (copper) exports, do commodity prices impact government spending on social programs?
  - Does reduced government spending on social programs lead to an increase in protest activity?

###### Data Collection: 
Data was collected using APIs and corresponding R packages, when available.  The following chart summarizes the data collected for this project, including the source and API or non-API method.  R code for accessing the APIs is below.

|    **Data     |    Source   |     Database |     API |R package|     R name**|
| :---------:      | :---------:      |:---------:       |:---------:      |:---------:         |:---------:        |
| GDP per capita, constant 2010 US$      | World Bank |World Development Indicators      |Yes       |WDI       |gdp.pc|
| Gini coefficient      | World Bank |World Development Indicators      |Yes       |WDI      |gini|
| Palma Ratio*     | World Bank |World Development Indicators      |Yes       |WDI       |palma|
| Central government debt, total (% of GDP)     | World Bank |World Development Indicators      |Yes       |WDI       |debt|
| Educational attainment, at least completed short-cycle tertiary (population 25+, total %)     | World Bank |World Development Indicators      |Yes      |WDI        |uni|
| Copper, ($/mt)      | World Bank |Commodities Markets "Pink Sheet" Data      |No       |N/A       |avg_cu_price|
| Social Spending, Public (% of GDP)     | OECD |Social Expenditure Database      |No*       |N/A       |social_exp|
| Pension Spending, Public (% of GDP)     | OECD |Social Expenditure Database      |No*      |N/A        |pension_exp|
| Public Unemployement Spending, Total (% of GDP)     | OECD |Social Expenditure Database      |No*      |N/A       |unemp_exp|
| General Government Deficit, Total (% of GDP)     | OECD |National Accounts at a Glance      |Yes      |OECD        |deficit|
| Mass Mobilization Protest Data    | Binghamton University |Harvard Dataverse      |No      |N/A       |protest_count|
| Twitter Data     | Twitter |N/A      |Yes      |rtweet        |N/A|

The R script to access the API data is below.
        
        ## World Bank API
        library(WDI)

        wdi.data = WDI(
          country = "CL",
          indicator = c("gini" = "SI.POV.GINI", "top10" = "SI.DST.10TH.10", 
                        "bottom20" = "SI.DST.FRST.20", "bottom10" = "SI.DST.FRST.10", "second20" = "SI.DST.02ND.20", "gdp.pc" = "NY.GDP.PCAP.KD",
                        "gov.uni" = "SE.XPD.TERT.ZS", "uni" = "SE.TER.CUAT.ST.ZS", "debt" = "GC.DOD.TOTL.GD.ZS"),
          start = 1970,
          end = 2020,
          extra = FALSE,
          cache = NULL)

        ## OECD API

        library(OECD)

        #see list of available datasets
        available_datasets_df <- get_datasets()

        #search for keyword in dataset
        as.data.frame(search_dataset("deficit"))

        deficit <- "SNA_TABLE12"
        filter_list <- list("CHL")
        def <- get_dataset(dataset = deficit, filter = filter_list)
        head(def)  

###### Methods: 
The R packages tidyverse/dpylr were used extensively to select, filter, and mutate variables, and join the various datasets into a single data frame for analysis.  This process was more challenging than expected, and more work should be done to further visualize the data.  R script for data cleaning and transformation is below.


        ##Filter and transform the data

        protest_chl <- dplyr::filter(protest, country=="Chile")

        protest_chl <- protest_chl %>%
          group_by(year) %>%
          summarize(protest_count = sum(protest))

        copper <- dplyr::select(commodity, year, Copper)

        copper <- copper %>%
          group_by(year) %>%
          summarize(avg_price = mean(Copper))

        wdiedit <- wdi.data %>%
          mutate(palma = top10 / (bottom20 + second20))

        names(def)[names(def) == "obsTime"] <- "year"

        names(oecd)[names(oecd) == "TIME"] <- "year"

        social_spend <- dplyr::filter(oecd, LOCATION=="CHL", INDICATOR=="SOCEXP", SUBJECT=="PUB", MEASURE=="PC_GDP")
        social_spend <-select(social_spend, year, Value)
        names(social_spend)[names(social_spend) == "Value"] <- "social_exp"

        pension_spend <- dplyr::filter(oecd, LOCATION=="CHL", INDICATOR=="PENSIONEXP", SUBJECT=="PUB", MEASURE=="PC_GDP")
        pension_spend <-select(pension_spend, year, Value)
        names(pension_spend)[names(pension_spend) == "Value"] <- "pension_exp"

        unemp_spend <- dplyr::filter(oecd, LOCATION=="CHL", INDICATOR=="PUBUNEMPEXP", SUBJECT=="TOT", MEASURE=="PC_GDP")
        unemp_spend <- select(unemp_spend, year, Value)
        names(unemp_spend)[names(unemp_spend) == "Value"] <- "unemp_exp"

        govdef <- dplyr::filter(deficit, LOCATION=="CHL")
        names(govdef)[names(govdef) == "TIME"] <- "year"
        names(govdef)[names(govdef) == "Value"] <- "deficit"
        govdef <- dplyr::select(govdef, year, deficit)

        wdi <- dplyr::select(wdiedit, year, gini, top10, bottom20, bottom10, second20, gdp.pc, gov.uni, uni, debt, palma)

        names(copper)[names(copper) == "avg_price"] <- "avg_cu_price"
        copper[, 1:ncol(copper)] <- lapply(1:ncol(copper), function(x) as.numeric(copper[[x]]))

        ## Merge datasets

        chl_data <- dplyr::full_join(social_spend, pension_spend, by = "year")
        chl_data <- dplyr::full_join(chl_data, unemp_spend, by = "year")
        chl_data <- dplyr::full_join(chl_data, protest_chl, by = "year")
        chl_data <- dplyr::full_join(chl_data, govdef, by = "year")
        chl_data <- dplyr::full_join(chl_data, wdi, by = "year")
        chl_data <- dplyr::full_join(chl_data, copper, by = "year") 


The R packages ggplot2 and patchwork were used for data visualization.  Below each image is the corresponding R script.

#### Visualization 1: Title
Key findings and any lit background

    ![](/images/gdppc.PNG)

R script

#### Visualization 2: Title
Key findings and any lit background
Chart
R script

#### Visualization 3: Title
Key findings and any lit background
Chart
R script

#### Part B: Twitter Data Analysis (In Progress)
###### Goal: 
Explore Twitter data from 3 key time periods to see how the key topics and sentiment has changed over time: 
- 2019 pre-protest
- Late October 2019 at the height of the protests
- Now

###### Data Selection: 
Because geographic data is limited by Twitter's privacy policies, I chose to evaluate mentions: @sebastianpinera, which is the Twitter account of Chile's President Sebastian Pinera; along with the boolean search, Chile AND inequality

###### Data Collection:
Data was collected using both the API and non-API method. 
- The python package GetOldTweets3 was used to source historical tweets, as Twitter's official API only allows you to scrape tweets from the last 7 days
  - Many issues exporting csv files; Google Collab seemed to work better until I received the error, "too many requests"
  - Need to narrow down my search; in progress
  - For now, I sourced a sample of tweets for a 6 month period before the protests
- For tweets within the last week, the R package rtweet was used.

The python and R codes are below.

###### R: 

        ##Load packages required for analysis

        library(rtweet)
        library(tidyverse)
        library(ggplot2)
        library(ggraph)
        library(translateR)
        library(igraph)
        library(wordcloud)
        library(data.table)

        ##Create Twitter API token

        token <- rtweet::create_token(
          app = "Advanced Data Programming",
          consumer_key <- "xxxxx",
          consumer_secret <- "xxxxx",
          access_token <- "xxxxx",
          access_secret <- "xxxxx")

        ##Search tweets for replies to Sebastian Pinera; ignore retweets

        pinera <- rtweet::search_tweets("@sebastianpinera", n = 10000, include_rts = FALSE, retryonratelimit = TRUE)
        save_as_csv(pinera, "pinera20.csv", prepend_ids = TRUE, na = "", fileEncoding = "UTF-8")

        ##Search tweets for Chile and inequality; ignore retweets

        i.tweet <- rtweet::search_tweets("chile AND desigualdad", n = 10000, include_rts = FALSE, retryonratelimit = TRUE)
        save_as_csv(i.tweet, "inequalitytweets.csv", prepend_ids = TRUE, na = "", fileEncoding = "UTF-8")

###### Python code in CMD: 
      --querysearch "sebastianpinera" --since 2019-MM-DD --until 2019-MM-DD --maxtweets 10000 --output pinera_month.csv

###### Methods: 
The historical twitter data was loaded in R, and the packages ggplot2, wordcloud, and igraph were used to analyze and visualize the data.  Below each image is the R script used for the visualization and any key observations.

#### Visualization 1: Title
Key findings and any lit background
Chart
R script

#### Visualization 2: Title
Key findings and any lit background
Chart
R script

### Future research: 
Because I am considering this as a potential dissertation topic, I plan to a) further develop the time series model and b) continue to collect rtweet data each week.
